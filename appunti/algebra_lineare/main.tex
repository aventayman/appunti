\documentclass[twoside]{report}

\input{preamble}
\input{macros}
\input{letterfonts}

\title{\Huge{Algebra Lineare e Geometria Analitica}\\Ingegneria dell'Automazione Industriale}
\author{\huge{Ayman Marpicati}}
\date{A.A. 2022/2023}
\setlength\parindent{0pt}

\begin{document}


\maketitle
\cleardoublepage
% \pdfbookmark[<level>]{<title>}{<dest>}
\pdfbookmark[section]{\contentsname}{toc}
\tableofcontents
\null\newpage

\setlength{\headheight}{15pt}

\pagestyle{fancy}
%... then configure it.
\fancyhead{} % clear all header fields
\fancyhead[LO]{\rightmark}
\fancyhead[RO]{\thepage}
\fancyhead[RE]{\leftmark}
\fancyhead[LE]{\thepage}
\fancyfoot{} % clear all footer fields

\chapter{Nozioni preliminari}
\section{Relazioni su un insieme}
\dfn{Relazione su un insieme}{Una \textbf{relazione} su un insieme \textit{A} è un qualunque sottoinsieme di \(\mcR\) del prodotto cartesiano \(A \times A\).

Una relazione \(\mcR\) su un insieme \textit{A} si dice:
\begin{itemize}
    \item \textbf{riflessiva} se, per ogni \(a \in A, \ a\mcR a\);
    \item \textbf{simmetrica} se, per ogni \(a,b \in A, \ a\mcR b \ \text{allora} \ a = b\);
    \item \textbf{antisimmetrica} se, per ogni \(a,b \in A, \ a\mcR b \text{ e } b\mcR a \text{ allora } a = b\);
    \item \textbf{transitiva} se, per ogni \(a,b,c \in A, \ a\mcR b \text{ e } b\mcR c \text{ allora } a \mcR c\);
\end{itemize}}

\dfn{Relazione d'ordine totale}{Una relazione d'ordine \(\mcR\) su un insieme \textit{A} si dice \textbf{relazione d'ordine} se è riflessiva, antisimmetrica e transitiva. Se inoltre, gli elementi di \textit{A} sono a due a due confrontabili, cioè, per ogni \(a, b \in A\), risulta \(a \mcR b\) oppure  \(b \mcR a\), la relazione \(\mcR\) si dice \textbf{relazione d'ordine totale}.}

\section{Strutture algebriche}
\dfn{Gruppo}{Sia \((G, \star)\) un insieme con un'operazione \(\star\). La struttura \((G, \star)\) si dice \textbf{gruppo} se:
\begin{itemize}
    \item l'operazione \(\star\)  è associativa;
    \item esiste in \textit{G} l'elemento neutro;
    \item ogni elemento di \(g \in G\) è simmetrizzabile.  
\end{itemize}
Se l'operazione \(\star\) soddisfa anche la proprietà commutativa, il gruppo si dice \textbf{abeliano}.}

\dfn{Campo}{Sia \textit{A} un insieme sul quale sono definite due operazioni che indichiamo con i simboli "\(+\)" e "\(\cdot\)" e che chiamiamo somma e prodotto rispettivamente. La struttura \((A, +, \cdot)\) è un \textbf{campo} se sussistono le condizioni seguenti:
\begin{itemize}
    \item \((A, +)\) è un gruppo abeliano il cui elemento neutro è indicato con 0;
    \item \((A\backslash\{0\}, \cdot)\) è un gruppo abeliano con elemento neutro \(e \neq 0\);
    \item valgono le proprietà distributive (sinistra e destra) del prodotto rispetto alla somma, cioè per ogni \(a,b,c \in A\) \[
        a \cdot (b + c) = a \cdot b + a \cdot c; \ (a + b) \cdot c = a \cdot c + b \cdot c
    \]
\end{itemize}}

\section{Matrici}

\dfn{Matrice}{Dato un campo K si dice \textbf{matrice} di tipo \(m \times n\) su \(K\) una tabella del tipo: \[ A=
\begin{pmatrix}
    a _{11} & a _{12} & \hdots & a _{1n} \\
    a _{21} & a _{22} & \hdots & a _{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a _{m1} & a _{m2} & \hdots & a _{mn} \\
\end{pmatrix}
\] avente \(m\) righe ed \(n\) colonne, i cui elementi \(a _{ij}\) sono elementi di \(K.\) }

\dfn{Matrice quadrata}{Una matrice di tipo \(n \times n\) è detta \textbf{matrice quadrata} di ordine \(n\). Queste vengono indicate con \(M_n(K)\).}

\dfn{Prodotto righe per colonne}{Date le matrici \(A = (a _{ih}) \in K ^{m,n}(K)\) con \(i \in I_m, h \in I_n\) e \(B = (b _{hj}) \in K ^{n,p}\) con \(h \in I_n, j \in I_p\), si dice \textbf{prodotto righe per colonne} di \(A\) per \(B\) la matrice \[
    A \cdot B = (c _{ij}) \text{ con } i \in I_m, \ j \in I_p \qquad \text{ove}
\] \[
    c _{ij} = a _{i1} b _{1j} + a _{i2} b _{2j} + ... + a _{in} b _{nj}= \sum_{h \in I_n} a _{ih} b _{hj}
\]    }

\ex{}{Prendiamo per esempio le due matrici: \[A=
\begin{pmatrix}
    -3 & 0 & 2 \\
    -4 & 7 & 1 \\
\end{pmatrix} \quad B=
\begin{pmatrix}
    -5 & -1 & 2 \\
    0 & 1 & -2 \\
    1 & 1 & 3 \\
\end{pmatrix}\]
 Il loro prodotto è \[
\begin{pmatrix}
    -3 \cdot (-5) + 0 \cdot 0 + 2 \cdot 1 & -3 \cdot (-1) + 0 \cdot 1 + 2 \cdot 1 & -3 \cdot 2 + 0 \cdot (-2) + 2 \cdot 3 \\
    -4 \cdot (-5) + 7 \cdot 0 + 1 \cdot 1 & -4 \cdot (-1) + 7 \cdot 1 + 1 \cdot 1 & -4 \cdot 2 + 7 \cdot (-2) + 1 \cdot 3 \\
\end{pmatrix}
\] Quindi \[
    A \cdot B =
\begin{pmatrix}
    17 & 5 & 0 \\
    21 & 12 & -19 \\
\end{pmatrix}
\]}
\dfn{Matrice identica}{L'elemento neutro delle matrici quadrate di ordine \(n\) è la \textbf{matrice identica}, cioè la matrice: \[
\begin{pmatrix}
    1 & 0 & \hdots & 0 \\
    0 & 1 & \hdots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \hdots & 1 \\
\end{pmatrix}
\]}

\dfn{Trasposta di una matrice}{Sia \(A = (a _{ij})\) una matrice di \(K ^{m,n}\). Si dice \textbf{trasposta} di \(A\) la matrice \(K^{n,m}\) ottenuta scambiando tra loro le righe con le colonne, cioè \(^{t}A = (b _{ji})\) ove \(b _{ji} = a _{ij}\) per ogni \(i \in I_n\) e \(j \in I_m.\)  }



\chapter{Spazi vettoriali}
\section{Generalità}
\dfn{Spazio vettoriale}{Siano \(K\)  un campo e \(V\) un insieme. Si dice che \(V\) è uno \textbf{spazio vettoriale} sul campo \(K\), se sono definite due operazioni: un'operazione interna binaria su \(V\), detta somma, \(+: V \times V \rightarrow V\) e un'operazione estrema detta prodotto esterno o prodotto per scalari, \(\cdot:K \times V \rightarrow V\), tali che
\begin{itemize}
    \item \((V, +)\) sia un gruppo abeliano;
    \item il prodotto esterno \(\cdot\) soddisfi le seguenti proprietà:
        \begin{itemize}
            \item \( (h \cdot k) \cdot v = h \cdot (k \cdot v) \quad \forall h, k \in K \quad e \quad \forall v \in V \) 
            \item \( (h + k) \cdot v = h \cdot v + k \cdot v \quad \forall h, k \in K \quad e \quad \forall v \in V\)
            \item \(h \cdot (v + w) = h \cdot v + h \cdot w \quad \forall h,k \in K \quad e \quad \forall v, w \in V\)
            \item \(1 \cdot v = v \quad \forall v \in V\)
        \end{itemize}
\end{itemize}}
Gli elementi dell'insieme \textit{V} sono detti \textbf{vettori}, gli elementi del campo \textit{K} sono chiamati \textbf{scalari}. L'elemento neutro di \((V, +)\) è detto \textbf{vettor nullo} e indicato \ul{0} per distinguerlo da 0, zero del campo \textit{K}. L'opposto di ogni vettore \textbf{v} viene indicato con \textbf{-v}.

\thm{}{Sia \textit{V} uno spazio vettoriale sul campo \textit{K}, siano \(k \in K\)  e \(v \in V\). Allora \[
    kv = \ul{0} \iff k = 0 \text{ oppure } v = \ul{0}
\]}
\pf{Dimostrazione}{Se k = 0 \[
    0v = (0+0)v = 0v + 0v
\] e sommando \(-0v\) ad ambo i membri si ottiene appunto \(\ul{0} = 0v\). Se è \(v = \ul{0}\), si procede nel modo analogo. Viceversa, se \(kv = \ul{0}\)  e \(k \neq 0\) dimostriamo che \(v = \ul{0}\). Dato che \(k \neq 0\), esiste l'inverso \(k ^{-1} \in K\) e, moltiplicando ambo i membri della precedente uguaglianza per \(k ^{-1}\) si ottiene \(k ^{-1}(kv) = k ^{-1} \ul{0}\) che, per quanto dimostrato in precedenza dà il \(\ul{0}\). Dato che \(k^{-1}(kv) = (k^{-1}k)v = 1v = v\), per la proprietà 4, si ha \(v = \ul{0}\).  }

\section{Sottospazi di uno spazio vettoriale}
\dfn{Sottospazio vettoriale}{Sia \(\emptyset \neq U \subseteq V\), diremo che \(U\) è \textbf{sottospazio vettoriale} di \(V\) se è esso stesso uno spazio vettoriale rispetto alla restrizione delle stesse operazioni.}

\mprop{Primo criterio di riconoscimento}{Sia \(V(K)\) uno spazio vettoriale e sia \(\emptyset \neq  U \subseteq V\) un suo sottoinsieme. Il sottoinsieme \(U\) è uno spazio vettoriale di \(V\) se, e soltanto se, sono verificate le seguenti condizioni:
\begin{enumerate}
    \item \(\forall u, u' \in U \quad u + u' \in U\) 
    \item \(\forall k \in K, \ \forall u \in U \quad ku \in U\) 
\end{enumerate}}

\mprop{Secondo criterio di riconoscimento}{Sia \(V(K)\) uno spazio vettoriale sul campo \(K\) e sia \(\emptyset \neq U \subseteq V\), \(U\) è sottospazio di \(V(K)\) se e soltanto se 
   \[
       hv_{1} + kv_{2} \in U \quad \forall v_{1}, v_{2} \in U \quad e \quad h, k \in K
   \] 
}

\section{Indipendenza e dipendenza lineare}
\dfn{Combinazione lineare}{Siano \(v _{1}, v_2, ..., v_n \in V(K)\) si dice combinazione lineare di vettori \(v_1, v_2, ..., v_n\) ogni vettore \(v\): \[
    v = k_1 \cdot v_1 + k_2 \cdot v_2 + ... + k_n \cdot v_n \quad \text{con} \ k_1, k_2, ..., k_n \in K
\] }
\dfn{Sistema di vettori libero}{Sia \(V(K)\) e sia \(A\) un sistema di vettori di \(V(K)\), \(A=[v_1, v_2, ..., v_n]\), allora \(A\) si dice \textbf{libero} se l'unica combinazione lineare di vettori di \(A\) che dà il vettore nullo è a coefficienti tutti nulli \[
    \ul{0} = k_1 \cdot v_1 + k_2 \cdot v_2 + ... + k_n \cdot v_n \implies k_1 = k_2 = ... = k_n = \ul{0}
\]
Se \(A\) è libero i suoi vettori si dicono \textbf{linearmente indipendenti}.}


\dfn{Sistema di vettori legato}{Sia \(V(K)\) e sia \(A\) un sistema di vettori di \(V(K)\), \(A=[v_1, v_2, ..., v_n]\), allora \(A\) si dice \textbf{legato} se \textbf{non} è libero. Quindi:\[
        \exists k_1, k_2, ..., k_n \ \text{non tutti nulli} : \ \ul{0}=k_1 \cdot v_1 + k_2 \cdot v_2 + ... + k_n \cdot v_n
\]
Se \(A\) è legato i suoi vettori si dicono \textbf{linearmente dipendenti}.}
Qui di seguito daremo delle proposizioni riguardo ai sistemi liberi e legati:
\mprop{}{Sia \(A=[v_1, v_2, ..., v_n]\) un sistema di generatori di \(V(K)\). Se \(\ul{0}\) appartiene ad \(A\), il sistema \(A\) è legato.}
\pf{Dimostrazione}{Sia \(\ul{0} \in A\), senza perdita di generalità, possiamo supporre che \(\ul{0} = v_1\) quindi: \[
    1 \cdot v_1 + 0 \cdot v_2 + ... + 0 \cdot v_n = 1 \cdot \ul{0} + \ul{0} = \ul{0} \implies \ \text{A è legato}
\]}
\mprop{}{Sia \(A=[v_1, v_2, ..., v_n]\) un sistema di generatori di \(V(K)\). Se in \(A\) appaiono due vettori proporzionali allora A è legato.}
\pf{Dimostrazione}{Senza perdita di generalità possiamo supporre che \(v_1 = k v_2\) e quindi: \[
    1v_1 + k v_2 + 0v_3 + ... + 0 v_n = v_1 - kv_2 + \ul{0} = \ul{0} \implies \ \text{A è legato}
\]}
\mprop{}{Sia \(A=[v_1, v_2, ..., v_n]\) un sistema di generatori di \(V(K)\). A è legato se e solo se almeno uno dei vettori si può riscrivere come combinazione lineare degli altri.}
\pf{Dimostrazione}{\( \implies \): Per ipotesi \(A\) è legato e quindi: \[
    \ul{0}=k_1v_1 + k_2 v_2 + ... + k_n v_n \ \text{con almeno un } k_i = 0
\] Senza perdita di generalità supponiamo che \(k_1 \neq 0\)
    \begin{gather*} 
    -k_1 v_1 = k_2 v_2 + ... + k_n v_n \qquad v_1 = \frac{1}{k_1} (-k_2 v_2 - ... -k_n v_n) \\
   v_1 = -\frac{k_2}{k_1}v_2 - \frac{k_3}{k_1}v_3 - ... - \frac{k_n}{k1}v_n
\end{gather*}
e quindi \(v_1\) è combinazione lineare di \(v_1, ..., v_n\).\\
\( \impliedby \): Per ipotesi uno dei vettori di \(A\) è combinazione lineare degli altri e senza perdita di generalità: \[
    v_1 = k_2 v_2 + k_3 v_3 + ... + k_n v_n \qquad \ul{0}= -1v_1 + k_2 v_2 + ... + k_n v_n
\] siccome \(-1 \neq 0\) \(A\) è legato. }
\mprop{}{Sia \(A=[v_1, v_2, ..., v_n]\) un sistema di generatori di \(V(K)\) e sia \(u \in V(K)\). Se \(A \cup \{u\}\) è legato, allora \(u\) è combinazione lineare dei vettori di \(A\).}
\pf{Dimostrazione}{Per ipotesi \(A \cup \{u\}\) è legato, cioè: \[
    \exists k_1, k_2, ..., k_n, b \in K \ \text{ non tutti nulli } : \ \ul{0} = k_1v_1 + k_2v_2 +...+k_nv_n + bu
\] sia per assurdo \(b = 0\) \[
    \ul{0}=k_1v_1 + k_2v_2 + ... + k_nv_n \text{ con } k_1 \neq 0 \ \implies \ A \text{ è legato, \textbf{assurdo!} } \implies \ b \neq 0
\] \[
    -bu = k_1v_1 + k_2v_2 + ... + k_nv_n \quad u = -\frac{k_1}{b}v_1 - \frac{k_2}{b}v_2 - ... - \frac{k_n}{b}v_n
\] \( \implies \) \(u\) è combinazione lineare dei vettori \(v_1, v_2, ..., v_n\)  }
\mprop{}{Sia \(A=[v_1, v_2, ..., v_n]\) un sistema di generatori di \(V(K)\) e sia \(B \supseteq A\) sistema di vettori di \(V(K)\). Se \(A\) è legato allora anche \(B\) è legato.}
\pf{Dimostrazione}{\[
    \exists k_1, k_2, ..., k_n \in K \ \text{ non tutti nulli } : \ \ul{0} = k_1v_1 + k_2v_2 +...+k_nv_n
\] Se \(B=[v_1, v_2, ..., v_n, w_1, w_2, ..., w_m]\) allora \[
    \ul{0} = k_1v_1 + k_2v_2 +...+k_nv_n + 0w_1 + 0w_2 + ... + 0w_m
\]\( \implies \ B\) è legato.}
\mprop{}{Sia \(A=[v_1, v_2, ..., v_n]\) un sistema di generatori di \(V(K)\) e sia \(B \subseteq A\) sistema di vettori di \(V(K)\), se \(A\) è libero, allora \(B\) è libero.}
\pf{Dimostrazione}{Sia, per assurdo, \(B\) legato, allora per la proposizione precedente anche \(A\) è legato. \textbf{Assurdo!} Quindi \(B\)  è libero.} 

\section{Sistemi di generatori di uno spazio vettoriale}
\dfn{Sistema di generatori}{Sia \(A\) sistema di vettori di \(V(K)\). \(A\) si dice sistema di generatori di \(V(K)\) se ogni \(v \in V(K)\) si può scrivener come combinazione lineare di un numero finito di vettori di A.}

\dfn{Copertura lineare}{Sia \(A\) un sistema di vettori di \(V(K)\) si dice copertura (o chiusura) lineare di \(A\) l'insieme \(\mcL(A)\) di tutte le combinazioni lineari di sottoinsiemi finiti di A.}
\nt{Dato \(A\) sistema di vettori di \(V(K)\) \begin{enumerate}
    \item \(\mcL(A)\) è il più piccolo sottospazio di \(V(K)\) che contiene \(A\) 
    \item \(\mcL(A) \le V(K)\) 
    \item \(\mcL(\mcL(A)) = \mcL(A)\)
\end{enumerate}}
Ogni spazio vettoriale ammette un sistema di generatori e:
\begin{itemize}
    \item se \(V(K)\) ammette un sistema di generatori finito \( \implies\) \(V(K)\) si dice finitamente generato.
    \item se ogni sistema di generatori di \(V(K)\) ha cardinalità infinita \( \implies\) \(V(K)\) non è finitamente generato.
\end{itemize}

\section{Basi e dimensione}
\mlenma{}{Sia \(S=[v_1, v_2, ..., v_n]\) un sistema di generatori per uno spazio vettoriale \(V(K)\), e sia \(v \in S\) combinazione lineare degli altri vettori (linearmente dipendente dagli altri) \( \implies\) \(S\backslash \{v\} \) è sistema di generatori per \(V(K)\)}
\pf{Dimostrazione}{Sia, senza perdere di generalità, \(v_1\) combinazione lineare di \(v_2, v_3, ..., v_n\) \[
    v_1 = k_2v_2 + k_3v_3 + ... + k_nv_n
\] sia \(v \in V(K)\) \[
    v = h_1v_1 + h_2 v_2 + ... + h_nv_n = h_1(k_2v_2 + ... + k_nv_n) + h_2v_2 + ... + h_nv_n
\] \[
v = \underbrace{(h_1k_2 + h_2)}_{\in K}v_2 + ... + \underbrace{(h_1k_n + h_n)}_{\in K}v_n \ \in \mcL([v_2, v_3, ..., v_n]) = \mcL(S \backslash \{v_1\} )
\] \( \implies \ S \backslash \{v_1\} \) è un sistema di generatori.}

\thm{}{Sia \(V(K)\) uno spazio vettoriale finitamente generato, non banale (\(V(K) \neq \{\ul{0}\} \)), allora esso ammette un sistema libero di generatori.}
\pf{Dimostrazione}{sia \(A = [v_1, v_2, ..., v_n]\) un sistema di generatori per \(V(K)\), abbiamo due possibilità: \begin{enumerate}
    \item A è libero \( \implies \) A è un sistema di generatori libero;
    \item A è legato \( \implies \ \exists v \in A\) combinazione lineare degli altri, senza perdita di generalità possiamo porre \(v = v_1 \ \implies \ A\backslash \{v_1\} = A_1 \) è sistema di generatori.
\end{enumerate} 
Se ci troviamo nel secondo caso possiamo reiterare il procedimento e trovare \(A_2 \rightarrow A_3 \rightarrow ...\) finché non arriviamo ad un sistema libero di generatori.

Osserviamo che \(A\) contiene almeno un \(v \in A: \ v \neq \ul{0}\), questo perché \(A_n = [0]\) e \(v_n \neq \ul{0}\) perché \(A \neq \{\ul{0} \} \ \implies \ A_n\) è necessariamente libero.  }

\dfn{Base}{Sia \(S = (v_1, v_2, ..., v_n)\) sequenza libera di vettori di \(V(K)\). \(S\) è detta base se e solo se \(S\) è una sequenza libera di generatori.}

\dfn{Base canonica di \(\RR^{n} \) }{\(((1,0,0,...,0)(0,1,0,...,0),...,(0,0,0,...,1))\) è una base canonica per \(\RR^{n} \).}

\mlenma{Lemma di Steinitz}{Sia \(V(K)\) uno spazio vettoriale finitamente generato. Sia \(B = [v_1, v_2, ..., v_n]\) sistema di generatori e \(A = [u_1, u_2, ..., u_m]\) sistema libero. Allora la cardinalità di A sarà sempre minore o uguale a quella del sistema di generatori. \((m \le n)\)  }

\pf{Dimostrazione}{Sia per assurdo \(m >n\), poiché \(B\) genera \(V(K)\) \(u_1\) si scrive come: \[
    u_1 = k_1v_1 + k_2 v_2 + ... + k_n v_n
\] Essendo \(A\) libero \(u_1 \neq \ul{0} \implies k_1, k_2, ..., k_n\) non sono tutti nulli \( \implies\) senza perdita di generalità \(k_1 \neq 0\) \[
    -k_1v_1 = -u_1 + k_2v_2 + ... + k_nv_n \qquad v_1 = \frac{1}{k_1}(u_1 - k_2v_2 - ... - k_nv_n)
\] \[
\implies v_1 \in \mcL([u_1, v_2, v_3, ..., v_n])
\] B è sistema di generatori, \(B \cup \{u_1\} \) è sistema di generatori, di conseguenza \((B \cup \{u_1\} \backslash \{v_1\} ) = B_1 = [u_1, v_2, ..., v_n]\) è ancora sistema di generatori per \(V(K)\).

Allo stesso modo posso riscrivere \[
    u_2 = \alpha u_1 + h_2v_2 + h_3 v_3 + ... + h_n v_n \quad \text{con} \ \alpha, h_2, h_3, ..., h_n \in K
\] Se avessimo \(h_2 = h_3 = ... = h_n = 0 \ u_2 = \alpha\) ma ciò non può succedere perché \(A\) è libero \( \implies \exists h_i \neq 0\) e senza perdita di generalità supporremo \(h_2 \neq 0\) quindi: \[
    -h_2 v_2 = \alpha u_1 - u_2 + h_3 v_3 + ... + h_n v_n \qquad v_2 = \frac{1}{h_2} (-\alpha u_1 + u_2 - h_3 v_3 - ... - h_n v_n)
\] \(v_2\) è linearmente dipendente da \(B_2 = [u_1, u_2, v_3, ..., v_n]\) e \(B_2\), per lo stesso motivo di \(B_1\) è ancora sistema di generatori.

Ora immaginiamoci di reiterare il procedimento \(n\) volte fino a trovare un sistema \(B_n = [u_1, u_2, ..., u_n]\). Siccome avevamo supposto che \(m > n\) essendo \(B_n\) sistema di generatori dovremo essere in grado di scrivere anche \(u _{n+1}\) come combinazione lineare dei vettori di \(B_n\), cioè: \[
    u _{n+1} \in \mcL(B_n) \qquad u _{n+1} = \alpha_1 u_1 + \alpha_2 u_2 + ... + \alpha_n u_n
\] questo comporta che \(A\) sia legato, ma questo è \textbf{assurdo!} \( \implies \ m \le n\).}

\thm{}{Sia \(V(K)\) uno spazio vettoriale finitamente generato, e siano \(B_1\) e \(B_2\) due sue basi le loro cardinalità sono uguali: \[
    B_1 = (v_1, v_2, ..., v_n) \qquad B_2 = (u_1, u_2, ..., u_n) \qquad m=n
\]} 
\pf{Dimostrazione}{Per dimostrarlo è sufficiente applicare il lemma di Steinitz \begin{itemize}
    \item \(B_1\) sistema di generatori, \(B_2\) sistema libero \( \implies \ n \ge m\); 
    \item \(B_2\) sistema di generatori, \(B_1\) sistema libero \( \implies \ m \ge n\). 
\end{itemize} \(m \ge n \text{ e } n \ge m \iff n = m\). }

\dfn{Dimensione}{Dato uno spazio vettoriale finitamente generato, non banale, chiamiamo \textbf{dimensione} di \(V\) la cardinalità di una qualsiasi delle sue basi. Inoltre se \(V = \{0\} \) poniamo la \(\dim(V)=0\) }

Qui di seguito enunciamo una serie di conseguenze del lemma di Steinitz.
\mprop{}{Sia \(V_n(K)\) uno spazio vettoriale di dimensione \(n\) su \(K\) e sia \(S=[v_1, v_2, ..., v_n]\) un sistema di generatori. Allora \(S\) è libero.}
\pf{Dimostrazione}{Sia \(B=[w_1, w_2, ..., w_n]\) una base di \(V_n(K)\). Sia per assurdo \(S\) legato.
Senza perdita di generalità \(v_1 = k_2v_2 + k_3v_3 + ... + k_nv_n\). Allora \(S' = S \backslash \{v_1\} \) è ancora sistema di generatori. \(|S'| = n-1 \ge |B|\) perché \(B\) è libero per il lemma di Steinitz. \textbf{Assurdo!}. Quindi \(S\) è libero.}

\mprop{}{Sia \(V(K)\) uno spazio vettoriale di dimensione \(n\) sul campo \(K\). Sia \(S=[v_1, v_2, ..., v_n]\) un sistema libero. Allora \(S\) è anche un sistema di generatori.}
\pf{Dimostrazione}{Sia \(B=[w_1, w_2, ..., w_n]\) una base di \(V(K)\), supponiamo per assurdo che \(S\) non generi. \[
    \implies \ \exists v \in V \text{ con } v \neq \ul{0} 
\] \(S' = S \cup \{u\} \) è ancora libero, supponiamo per assurdo che non lo sia: \[
    \text{ sia } \ul{0} = k_1v_1 + k_2v_2 + ... + k_nv_n + \alpha v \text{ con } \alpha \neq 0
\] \[
    \text{ altrimenti avremmo: } \ul{0} = k_1v_1 + k_2v_2 + ... + k_nv_n
\] \[
    v = \frac{1}{\alpha}(-k_1v_1 - k_2 v_2 - ... - k_nv_n) \in \mcL(S)
\] \( \implies v \in \mcL(S)\) \textbf{assurdo!} Contro l'ipotesi che \(v \notin \mcL(S) \ \implies \ S'\) è libero. \[
\underbrace{|S'| = n+1}_{\text{sistema libero}} \le \underbrace{|B| = n}_{\text{sequenza di generatori}} \rightarrow \text{ per il lemma di Steinitz }
\] \textbf{Assurdo!} \( \implies\) \(S\) è un sistema di generatori.}

\mprop{}{\(m\) vettori in \(V_n(K)\) con \(m > n\) sono sempre linearmente dipendenti.}
\pf{Dimostrazione}{Siano per assurdo [\(v_1, v_2, ..., v_m\)], \(m\) vettori linearmente indipendenti con \(m > n\). Sia \(B\) una base di \(V_n(K)\). \(m = |S=[v_1, v_2, ..., v_m]| \le |B| = n\) per il lemma di Steinitz. Ma per ipotesi \(m > n\), \textbf{assurdo!} }

\mprop{}{\(m\) vettori in \(V_n(K)\) con \(m < n \implies\) non possono generare. }
\pf{Dimostrazione}{siano \(v_1, v_2, ..., v_m\) per assurdo \(m\) vettori che generano \(V_n(K)\) con \(m < n\) allora: \[m = |S=[v_1, v_2, ..., v_n]| \ge |B|=n \ \text{ con } \ m \ge n \quad \text{per il lemma di Steinitz}\] \textbf{Assurdo!} Va contro all'ipotesi.}

\thm{Teorema di caratterizzazione delle basi}{Sia \(B=(v_1, v_2,...,v_n)\) una sequenza di vettori di \(V(K)\). \(B\) è una base se e solo se ogni vettore di \(V\) si può scrivere in maniera univoca come combinazione lineare dei vettori di \(B\). \[
    \forall v \in V,\ \exists ! \ v = k_1v_1 + k_2 v_2 + ... + k_n v_n \quad k_i \in K
\]}
\pf{Dimostrazione}{
\( \implies\) sia \(B\) una base di \(V\). Per ogni \(v\) si ha che \(v \in \mcL(B)\) perché \(B\) è una sequenza di generatori. Supponiamo per assurdo che esista \(v \in V\): \[
    v = v = k_1v_1 + k_2 v_2 + ... + k_n v_n = h_1 v_1 + h_2 v_2 + ... + h_n v_n \quad \text{ con almeno un  } k_i \neq h_i 
\] \[
(k_1 - h_1) v_1 + (k_2 - h_2) v_2 + ... + (k_n - h_n) v_n = \ul{0} 
\] \(B\) è una sequenza libera, quindi \((k_i - h_i) = 0 \implies k_i = h_i\) perché l'unica combinazione lineare che dà il vettore nullo è quella a coefficienti tutti nulli.
Ma avevamo supposto che \(k_i \neq h_i \implies\) \textbf{assurdo!} \( \implies \exists!\) la combinazione lineare dei vettori di \(B\) che dà \(v \ (\forall v \in V)\).

\( \impliedby\) per ipotesi \(\forall v \in V \ \exists !\) combinazione lineare dei vettori di \(B\) che dà \(v\). \(B\) è una sequenza di generatori, cioè \(\forall v \in V \implies v \in \mcL(B)\). Supponiamo per assurdo che \(B\) sia legato \(\implies \exists k_i \in K\) non nullo: \[
    \ul{0} = k_1v_1 + k_2 v_2 + ... + k_n v_n \quad \ul{0} = 0v_1 + 0v_2 + ... + 0v_n
\] quindi esistono almeno due combinazioni lineari di \(B\) che danno \(\ul{0} \). Dato che \(\ul{0} \in V\) per ipotesi esiste un unica combinazione lineare dei vettori di \(B\) che dà \(\ul{0} \). \textbf{Assurdo!} Quindi \(B\) è una sequenza libera e \(B\) è una base per \(V\).}

\dfn{Componenti di un vettore rispetto ad una base}{Sia \(B=(v_1, v_2, ..., v_n)\) una base di \(V_n(K)\) e sia \(v \in V\). Chiameremo componenti di \(v\) rispetto alla base \(B\) la sequenza \((k_1, k_2, ..., k_n)\): \[
    v = k_1v_1 + k_2 v_2 + ... + k_nv_n
\]}

\mprop{}{Sia \(V_n(K)\) uno spazio vettoriale di dimensione \(n\) sul campo \(K\), allora \(V_n(K)\) ammette almeno un sottospazio di dimensione \(m\) \(\forall 0 \le m \le n\). }
\pf{Dimostrazione}{sia  \(B=(v_1, v_2,...,v_n)\) una base di \(V_n(K)\) e sia \(0 \le m \le n\), ci sono due possibilità: \begin{enumerate}
    \item \(m=0 \implies\) \{\ul{0} \} è il sottospazio voluto;
    \item \(0 < m \le n\) e quindi \(S=(v_1, v_2,...,v_m)\)
\end{enumerate} \(\mcL(S)\) ha dimensione \(m\) perché \(S\) è libero \((S \subseteq B)\) e genera, per definizione \(\mcL(S)\).}

\newpage
\mprop{}{Siano \(U, W \le V_n(K)\) e sia \(U \le W\), allora: \begin{enumerate}
    \item \(\dim(U) \le \dim(W)\)
    \item \(U = W \iff \dim(U) = \dim(W)\)
\end{enumerate}}
\pf{Dimostrazione}{Dimostriamo i due punti:\begin{enumerate}
        \item Sia \(B\) base per \(U\) e \(B'\) base per \(W\), se per assurdo \[\underbrace{\dim(U) = |B|}_{\text{sequenza libera di \(W\) }} > \underbrace{\dim(W) = |B'|}_{\text{genera \(W\)} } \] contro il lemma di Steinitz. 
        \item \( \implies\) è banale; \\
        \( \impliedby\) sia per assurdo \(U < W\) e sia \(B\) base di \(U\), allora \[
            |B| = \dim(U) = \dim(W)
        \] quindi \(B\) è una base anche per \(W\) \( \implies \mcL(B) = W \implies W = U\) \textbf{Assurdo!}
\end{enumerate}}

\thm{Teorema del completamento ad una base}{Sia \(V_n(K)\) uno spazio vettoriale di dimensione \(n\) e sia \(A=(v_1, v_2,...,v_p)\), ove \(p \le n\), una sequenza libera di vettori in \(V_n(K)\). Allora, in una qualunque base di \(B\) di \(V_n(K)\), esiste una sequenza \(B'\) di vettori, tale che \(A \cup B'\) è una base di \(V_n(K)\).}

\section{Intersezione e somma di sottospazi}
\mprop{}{Sia \(V_n(K)\) uno spazio vettoriale di dimensione \(n\) sul campo \(K\) e siano \(U, V \le V \implies U\cap W\) è sottospazio di \(V\).}
\pf{Dimostrazione}{Richiamo il secondo criterio di riconoscimento dei sottospazi. \(U \cap W\) è un sottospazio di \(V\) \(\iff\) è sottoinsieme non vuoto di \(V\): \[
    \forall v_1, v_2 \in U \cap W, \ \forall k_1, k_2 \in K, \ k_1 v_1 + k_2v_2 \in U \cap W
\] \(U \cap W\) è sottoinsieme non vuoto di \(V\), perché \(U \subseteq V\), \(W \subseteq V\) e \(\ul{0} \in U \cap W\). Siano ora \(v_1, v_2 \in U \cap W\) e \(k_1, k_2 \in K\), osserviamo per il secondo criterio di riconoscimento che \(k_1 v_1 + k_2 v_2 \in U\) e per lo stesso motivo \(k_1 v_1 + k_2 v_2 \in W\) \( \implies k_1 v_1 + k_2 v_2 \in U \cap W \implies U \cap W\) è un sottospazio vettoriale.}

\nt{Sotto le stesse ipotesi della proposizione precedente abbiamo che \(U \cup W\) non è un sottospazio a meno che \(U \subseteq W\) oppure \(W \subseteq U\).}

\dfn{Spazio di somma}{Dati \(U\) e \(W \le V\) spazio vettoriale di dimensione \(n\) su \(K\) definiamo lo \textbf{spazio di somma} come: \[U + W := \{u + w \ | \ u \in U \ e \ w \in W\} \]}

\mprop{}{Dati \(U\) e \(W \le V\) spazio vettoriale di dimensione \(n\) su \(K\) abbiamo che: \(U + W \le V\) }
\pf{Dimostrazione}{Osserviamo che \(U + W \subseteq V\) perché dato \(u \in U \) e \(w \in W\), \(u \in V\) e \(w \in V \implies u + w \in V\), il quale non è vuoto perché \(\ul{0} \in U + W\). Siano \(v_1, v_2 \in U + W\) e siano \(k_1, k_2 \in K\) \[
    k_1 \cdot \underbrace{v_1}_{\text{\(=u_1 + w_1\) }}  + k_2 \cdot \underbrace{v_2}_{\text{\(= u_2 + w_2\) }}  = k_1(u_1 + w_1) + k_2(u_2 + w_2) = \underbrace{(k_1 u_1 + k_1 w_1)}_{\text{\(u_3 \in U\) per il 2° criterio}} + \underbrace{ (k_2 u_2 + k_2 w_2)}_{\text{\(w_3 \in W\) per il 2° criterio}}  
\] \[
    \implies u_3 + w_3 \in U + W \implies \text{ per il 2° criterio } \ U + W \le V
\]}

\mprop{}{Siano \(U, W \le V_n(K)\) allora \(U + W\) è il più piccolo sottospazio di \(V\) che cotiene \(U \cup W\); equivalentemente \[\mcL(U \cup W) = U + W\]}
\dfn{Somma diretta}{Dati \(U, W \le V_n(K)\) diremo che \(U + W\) è somma diretta se \(\forall v \in U + W\) può essere scritto come unico modo come \(u + w\). Equivalentemente \[
    \forall v \in U + W \quad \exists ! \ u \in U \ e \ w \in W : \quad v = u + w
\]Se \(U + W\) è una somma diretta allora la indicheremo con \(U \oplus W\). }

\mprop{}{Siano \(U, W \le V_n(K)\) allora \(U \oplus W \iff U \cap W = \{\ul{0} \} \).}
\pf{Dimostrazione}{\( \implies\) Siano \(U, W\) in somma diretta e sia, per assurdo: \(x \in U \cap W\) con \(x \neq \ul{0} \). Sia \(v = u + w\) con \(u \in U \ e \ w \in W\). Consideriamo \[
    v + x - x = v \implies v = u + w + x - x = \underbrace{u + x}_{\in U} + \underbrace{w - x}_{\in W} = u_1 + w_1
\] \[
    u = u + x \quad e \quad w = w - x \text{ poiché la somma è diretta } \implies x = \ul{0} \implies \text{\textbf{Assurdo!}} \implies U \cap W = \{\ul{0} \} 
\] \( \impliedby\) Siano \(U, W: \ U \cap W = \{\ul{0} \}\) e supponiamo per assurdo che esista \(v \in U + W:\) \[
    v = u_1 + w_1 \quad e \quad v = u_2 + w_2 \qquad \text{ con } \ u_1, u_2 \in U \quad e \quad w_1, w_2 \in W \quad e \quad (u_1, w_1) \neq (u_2, w_2)
\] \[
    u_1 + w_1 = u_2 + w_2 \quad v_2 = \underbrace{u_1 - u_2}_{\in U} = \underbrace{w_2 - w_1}_{\in W} \in U \cap W
\] \[
    \implies u_1 - u_2 = \ul{0} \quad e \quad w_2 - w_1 = \ul{0}
\] \[
    \implies u_1 = u_2 \quad e \quad w_1 = w_2
\] che è \textbf{assurdo!} Questo perché avevamo supposto che \(v\) avesse due scritture distinte come somma i elementi di \(U \ e \ W.\) \[
    \implies \exists ! \ (u_1, w_1): \quad u, \in U \quad e \quad w_1 \in W: \quad v=u_1 + w_1 \ e \ U \oplus W 
\]}

\cor{}{Siano \(U, W \le V_n(K)\) allora \(V = U \oplus W \iff U + W = V \ e \ U \cap W = \{\ul{0} \} \).}
\nt{Siano \(U, W \le V_n(K)\) e sia \(B_1\) una base di \(V\) e \(B_2\) una base di \(W\) \( \implies B_1 \cup B_2\) è sequenza di generatori per lo spazio \(U + W\). In generale l'unione di due basi, non è a sua volta una base per \(U + W.\) }

\mprop{}{Siano \(U, M \le V_n(K): U \oplus W\) e sia \(A\) una sequenza libera di vettori di \(U\) e \(B\) una sequenza libera di vettori di \(U\). Allora \(A \cup B\) è una sequenza libera di vettori della \(U \oplus W\).}
\pf{Dimostrazione}{Siano \(A = (u_1, u_2, ..., u_k)\) e \(B = (w_1, w_2, ..., w_h)\) e supponiamo per assurdo che \(a_1, a_2, ..., a_k \in K\) e \( b_1, b_2, ..., b_h \in K\), quindi per assurdo sia legata la combinazione lineare: \[
    \ul{0} = a_1 u_1 + a_2 u_2 + ... + a_k u_k + b_1 w_1 + b_2 w_2 + ... + b_h w_h \ \text{ non tutti nulli }
\] \[
    \underbrace{-(a_1 u_1 + a_2 u_2 + ... + a_k u_k)}_{ \in U } = \underbrace{b_1 w_1 + b_2 w_2 + ... + b_h w_h}_{\in W}
\] \[
    \implies \ul{0} = b_1 w_1 + b_2 w_2 + ... + b_h w_h \quad e \quad \ul{0} = a_1 u_1 + a_2 u_2 + ... + a_k w_k
\]ma \(A\) e \(B\) sono sequenze libere quindi \(a_1 = a_2 = ... = a_k = 0 \quad e \quad b_1 = b_2 = ... = b_h = 0\) 
\[ \implies \nexists a_1, a_2, ..., a_k, b_1, b_2, ..., b_h \text{ non tutti nulli: } \] 
\[
    \ul{0} = a_1 u_1 + a_2 u_2 + ... + a_k u_k + b_1 w_1 + b_2 w_2 + ... + b_h w_h \implies \text{\textbf{Assurdo!}}
\] \( \implies A \cup B\) è una sequenza libera. 
}

\cor{}{Siano \(U, W \in V_n(K): U \oplus W\) e siano \(B_U\) e \(B_W\) basi di \(U\) e \(W\) \( \implies B_U \cup B_W\) è una base per \(U \oplus W\). }

\mprop{Formula di Grassmann}{Dati \(U, W \le V_n(K)\) abbiamo che: \[
    \dim(U + W) + \dim(U \cap W) = \dim(U) + \dim(W)
\]}
\dfn{Complemento diretto}{Sia \(W \le V_n(K)\) si dice \textbf{complemento diretto} di \(W\) in \(V\) uno spazio \(U \le V: U \oplus W = V.\) }
\nt{Un complemento diretto di \(W\) in \(V\) esiste sempre e si trova estendendo una base di \(W\) a una base di \(V\). In generale questo non è unico.}

\chapter{Sistemi lineari}
\section{Determinante di una matrice quadrata}
\dfn{Determinante}{Sia \(A = (a _{ij})\) una matrice quadrata, di ordine \(n\), a elementi in un campo \(K.\) Si dice \textbf{determinante} di \(A\), e si scrive \(|A|\) oppure \(\det(A)\), l'elemento di \(K\) definito ricorsivamente come segue: \begin{enumerate}
    \item se \(n = 1 \qquad A = (a _{11}) \qquad \det(A) = |A| = a _{11}\) 
    \item se \(n > 1 \qquad A = a _{ij} \qquad \det(A)=(-1)^{1+1} a _{11} \det A _{11} + (-1)^{1+2} a _{12} \det A _{12} + ... + (-1)^{1 + n} a _{1n} \det A _{1n}\) 
\end{enumerate}}

Se \(A = \begin{pmatrix} a _{11} & a _{12} \\ a _{21} & a _{22} \end{pmatrix} \), il suo determinante è \(|A| = a _{11} a _{22} - a _{12} a _{21}\).

Mentre se \[A = 
\begin{pmatrix}
    a_{11} & a _{12} & a _{13} \\
    a _{21} & a _{22} & a _{23} \\
    a _{31} & a _{32} & a _{33} \\
\end{pmatrix}
\]
Allora la il determinante di \(A\) è \[
    |A| = a _{11} a _{22} a_{33} + a_{13} a_{21} a_{32} + a_{12} a_{23} a_{31} - a_{13} a_{22} a_{32} - a_{11} a_{23} a_{32} - a_{12} a_{21} a_{33}  
\]
\dfn{Complemento algebrico}{Sia \(A = (a_{ij} )\) una matrice quadrata di ordine \(n\), a elementi in campo \(K\). Si dice \textbf{complemento algebrico} dell'elemento \(a_{hk} \), e si indica \(\Gamma_{hk} \), il determinante della matrice quadrata di ordine \(n -1\), ottenuta da \(A\) sopprimendo la h-esima riga e la k-esima colonna, preso con il segno \((-1)^{h+k} \). }
\thm{Primo teorema di Laplace}{Data la matrice quadrata di ordine \(n\), la somma dei prodotti degli elementi di una sua riga (o colonna), per i rispettivi complementi algebrici, è il determinante di \(A.\) }
Pertanto, la formula per il calcolo del determinante di \(A = (a_{ij} )\) rispetto alla a i-esima riga è \[
    |A| = \sum_{j = 1}^{n} a_{ij} \Gamma _{ij} \qquad \forall i = 1,2,..., n
\] rispetto alla j-esima colonna è \[
    |A| = \sum_{i = 1}^{n} a_{ij} \Gamma _{ij} \qquad \forall j = 1,2,..., n
\]

\thm{Secondo teorema di Laplace}{Sia \(A\) una matrice quadrata di ordine \(n\). La somma dei prodotti degli elementi di una sua riga (o colonna) per i complementi algebrici degli elementi di un'altra riga (o colonna) vale zero. Quindi \[
    A \in M_n(K) \implies
\begin{cases}
    a_{i1} \Gamma_{j1} + a_{i2} \Gamma_{j2} + ... + a_{in} \Gamma_{jn} = 0 \quad i \neq j \\
    a_{1i} \Gamma_{1j} + a_{2i} \Gamma_{2j} + ... + a_{ni} \Gamma_{nj} = 0 \quad i \neq j \\
\end{cases}
\]}

\thm{Teorema di Bidet}{Date due matrici quadrate di ordine \(n\), \(A\) e \(B\), il determinante della matrice prodotto \(A \cdot B\) è uguale al prodotto dei determinanti di \(A\) e \(B\), cioè \[
    |A \cdot B| = |A| |B| 
\] }

\section{Matrici invertibili}
\dfn{Matrice invertibile}{Una matrice quadrata, di ordine \(n\), si dice \textbf{invertibile} quando esiste una matrice \(B\), quadrata e dello stesso ordine, tale che \(A \cdot B = B \cdot A = I_n\), dove \(I_n\) è la matrice identica di ordine \(n\). La matrice \(B\) si dice \textbf{inversa} di \(A\) e si indica \(A^{-1} \).}

\thm{}{Sia \(A \in M_n(K)\); allora \(A\) è invertibile \( \iff |A| \neq 0\) e in tal caso \[
    A^{-1} = \frac{1}{|A| }\left.^tA_a\right.
\] dove \(A_a\) si chiama \textbf{matrice aggiunta} di \(A\) ed è la matrice ottenuta da \(A\) sostituendo ogni elemento con il suo complemento algebrico \(\Gamma\). }

\section{Dipendenza lineare e determinanti}
\dfn{Minore}{Sia \(A \in K^{m,n} \). Si chiama \textbf{minore di ordine \(p\)} estratto da \(A\), con \(p \in \mathbb{N}\), \(p \neq 0\), \(p \le \min \{m,n\} \), una matrice quadrata di ordine \(p\) ottenuta cancellando \(m-p\) righe e \(n-p\) colonna da \(A\). }

\thm{}{ Una sequenza \(S=(v_1,v_2, \ldots,v_n)\) di \(n\) vettori dello spazio vettoriale \(V_n(K)\) è libera se, e soltanto se, la matrice \(A\), che ha nelle proprie righe (o colonne) le componenti dei vettori di \(S\) in una base di \(V_n(K)\), ha determinante non nullo ed è legata se, e soltanto se, tale matrice \(A\) ha determinante nullo.}

\dfn{Rango di una matrice}{Sia \(A\) una matrice di \(K^{m,n}(K)\). Si dice \textbf{rango} della matrice \(A\), e si scrive \(\rho (A)\), l'ordine massimo di un minore estraibile da \(A\) con determinante non nullo.}

\newpage
\paragraph{Osservazione:} Data la matrice \(A\) di \(K^{m,n}(K)\)
\begin{enumerate}
    \item \(\rho (A)=0 \iff A\) è la matrice nulla;
    \item \(\rho (A) = \rho (^{t}A)\) ;
    \item \(\rho (A) \le \min(m,n)\).
\end{enumerate}

\dfn{Spazio delle righe e delle colonne}{Data una matrice \(A\), avente \(m\) righe ed \(n\) colonne, si dice \textbf{spazio delle righe} di \(A\), e si indica \(\mcL(R) \), il sottospazio \(K^{n}(K)\) generato dalle righe di \(A\). Si dice  \textbf{spazio delle colonne} di \(A\), e si indica \(\mcL(C) \), il sottospazio vettoriale di \(K^{m}(K)\) generato dalle colonne di \(A\).}

\thm{Teorema di Kronecker}{Gli spazi vettoriali \(\mcL(R) \) ed \(\mcL(C)\), di una matrice \(A \in K^{m,n}(K)\), hanno la stessa dimensione e tale dimensione coincide con il rango di \(A\). Cioè: \[
\dim(\mcL(R) ) = \dim(\mcL(C) ) = \rho (A)
.\] }
\pf{Dimostrazione}{Dimostriamo che \(\dim(\mcL(R) ) = \rho (A)\). La dimostrazione per quanto riguarda le colonne è completamente analoga. Sia \(s = \dim(\mcL(R) )\implies\) abbiamo \(s\) righe linearmente indipendenti nella matrice \(A\) e quindi per il teorema precedente esiste un minore in \(A\) di ordine \(s\) a determinante non nullo. Pertanto \(\rho (A) \ge s\). Sia per assurdo \(\rho (A) = r > s\), dovrebbe esistere in \(A\) un minore di ordine \(r\) a determinante non nullo. Se chiamiamo ora \(S = (R_1, R_2, \ldots, R_r)\) la sequenza di righe nella matrice \(A\), la matrice \(A\) ha un minore di ordine \(r\) non singolare e di conseguenza è libera. Quindi \[
\dim \mcL(R) \ge \dim \mcL(S) = r > s = \dim \mcL(R) 
.\] Ma questo è un \textbf{assurdo!} Quindi \[
\rho (A) = r \le s = \dim \mcL(R) \implies r = s 
.\]}

\cor{}{Se \(A\) è una matrice quadrata di ordine \(n\), con elementi in un campo \(K\), le sequent condizioni sono equivalenti:
\begin{enumerate}
    \item \(|A| \neq 0\) ;
    \item \(A\) è invertibile;
    \item \(\rho (A) = n\) ;
    \item le righe sono linearmente indipendenti e, quindi, sono base di \(K^{n}\);
    \item le colonne sono linearmente indipendenti e, quindi, sono base di \(K^{n}\).
\end{enumerate}}

\thm{Teorema degli orlati}{Una matrice \(A \in K^{m,n}(K)\) ha rango \(p\) se, e solo se, esiste un minore \(M\) di ordine \(p\) a determinante non nullo e tutti i minori di ordine \(p + 1\), che contengono \(M\), hanno determinante nullo.}

\section{Sistemi lineari}
\dfn{Sistema lineare}{Un \textbf{sistema lineare} è un insieme di \(m\) equazioni lineari in \(n\) incognite a coefficienti in campo \(K\).} Un sistema lineare si può, quindi, indicare nel modo seguente: \[
\begin{cases}
    \ a_{11}x_1+a_{12}x_2+\ldots +a_{1n}x_n = b_1 \\
    \ a_{21}x_1+a_{22}x_2+\ldots +a_{2n}x_n = b_2 \\
    \ \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots  \\
    \ a_{m_1}x_1+a_{m_2}x_2+\ldots +a_{mn}x_n = b_m \\
\end{cases}
\] con \(a_{ij}, b_l \in K\). Gli elementi \(a_{ij}\) si chiamano coefficienti delle incognite, gli elementi \(b_l\) si dicono termini noti.

La matrice \(m \times n\) \[
A =
\begin{pmatrix}
    a_{11} & a_{12} & \ldots  & a_{1n} \\
    a_{21} & a_{22} & \ldots  & a_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{m1} & a_{m 2} & \ldots  & a_{mn} \\
\end{pmatrix}
\] è detta matrice dei coefficienti o \textbf{matrice incompleta}, la matrice \(n \times 1\) \[
X =
\begin{pmatrix}
    x_1 \\
    x_2 \\
    \vdots \\
    x_n \\
\end{pmatrix}
\] è detta delle matrice colonna delle incognite, mentre la matrice \(m\times 1\) \[
B = 
\begin{pmatrix}
    b_1 \\
    b_2 \\
    \vdots \\
    b_m \\
\end{pmatrix}
\] è detta matrice colonna dei termini noti. La matrice \(m \times (n+1)\) \[
A | B = 
\begin{pmatrix}
    a_{11} & \ldots  & a_{1n} & b_1 \\
    a_{21} & \ldots  & a_{2n} & b_2 \\
    \vdots & \ddots & \vdots & \vdots \\
    a_{m1} & \ldots  & a_{mn} & b_m \\
\end{pmatrix}
\] è detta \textbf{matrice completa}.
Infine, il sistema iniziale si può riscrivere come: \(A \cdot X = B\), cioè \[
\begin{pmatrix}
    a_{11} & a_{12} & \ldots  & a_{1n} \\
    a_{21} & a_{22} & \ldots  & a_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{m1} & a_{m 2} & \ldots  & a_{mn} \\
\end{pmatrix}
\begin{pmatrix}
    x_1 \\
    x_2 \\
    \vdots \\
    x_n \\
\end{pmatrix}
=
\begin{pmatrix}
    b_1 \\
    b_2 \\
    \vdots \\
    b_m \\
\end{pmatrix}
\] 

\dfn{Sistema omogeneo}{Un sistema lineare si dice \textbf{omogeneo} quando tutti i termini noti sono nulli. \[
AX = \ul{0} 
\] }

\paragraph{Osservazione:} Data \(A \in K^{m,n} \quad A = 
\begin{pmatrix}
    C_1 & C_2 & \ldots & C_n \\
\end{pmatrix}
\) ove le colonne \(C_j\) sono vettori di \(K^{m,1}\) e quindi utilizziamo utilizzando questa notazione il sistema si può scrivere come \[
x_1C_1 + x_2C_2+\ldots +x_nC_n = B
\] 

\dfn{Sistema compatibile}{Un sistema lineare in \(m\) equazioni ed \(n\) incognite ha soluzione, ovvero si dice che il sistema è \textbf{compatibile}, se esiste almeno una n-upla  \(\alpha _1, \alpha _2, \ldots , \alpha _n\) di elementi di \(K\) che risolve tutte le equazioni del sistema. Tale n-upla è detta \textbf{soluzione}.}

\paragraph{Osservazione:} Posto \(A = (C_1, C_2, \ldots , C_n)\) \[
A 
\begin{pmatrix}
    \alpha _1 \\
    \alpha _2 \\
    \vdots \\
    \alpha _n \\
\end{pmatrix}=B \iff 
\alpha _1 C_1 + \alpha _2 C_2 + \ldots + \alpha _n C_n = B
\] che è equivalente a dire che \(B\) è combinazione lineare delle colonne di \(A\). Quindi il sistema è risolubile se, e soltanto se, \(B \in \mcL(C_1, C_2, \ldots ,C_n) \).

\thm{Teorema di Rouché-Capelli}{Un sistema lineare \(A X = B\)
 è compatibile se, e soltanto se, \(\rho (A) = \rho (A|B)\).}

 \pf{Dimostrazione}{"\(\implies \)" Sia \(AX = B\) risolubile, \(\implies \exists \ (\alpha _1, \alpha _2, \ldots ,\alpha _n) : \ \alpha _1 C_1 + \alpha _2 C_2 + \ldots + \alpha _n C_n = B\) quindi 
\[ B \in \mcL(C_1, C_2, \ldots , C_n) \implies \underbrace{\dim \mcL(C_1, C_2, \ldots ,C_n, B)}_{= \rho (A|B)} = \underbrace{\dim \mcL(C_1, C_2, \ldots ,C_n)   }_{= \rho (A)} \]
\[\implies \rho (A|B) = \rho (A)\] 
 "\(\impliedby \)" Per ipotesi abbiamo che \(\rho (A|B) = \rho (A)\). Quindi 
\[
\dim \mcL(C_1, C_2, \ldots ,C_n, B) = \dim \mcL(C_1, C_2, \ldots ,C_n) \implies \mcL(C_1, C_2, \ldots ,C_n, B) = \mcL(C_1, C_2, \ldots ,C_n)\]
\[\implies B \in \mcL(C_1, C_2, \ldots ,C_n)  
    \]\[\implies  \exists (k_1,k_2, \ldots ,k_n): \ k_1C_1+k_2C_2+\ldots +k_nC_n = B\]
Quindi la n-upla \((k_1, k_2, \ldots , k_n)\) è soluzione di \(AX = B\) e di conseguenza il sistema è compatibile.}

\thm{Teorema di Cramer}{Sia \(AX = B\) un sistema lineare in \(n\) equazioni ed \(n\) incognite. Se \(\det(A) \neq 0\) allora \(AX = B\) ammette un'unica soluzione.}
Indichiamo con \(B_1\), la matrice ottenuta sostituendo a \(C_i\) la colonna dei termini noti (\(B\)).
\[
A = (C_1, C_2, \ldots , C_n) \quad B_1 = (C_1, C_2, \ldots , C_{i-1}, B, C_{i+1}, \ldots , C_n)
\] Se \(\det (A) \neq 0\) allora (\(X_1, X_2, \ldots , X_n\)) è data da: \[
X_1 = \frac{|B_1| }{|A| }=\frac{\det(B_1)}{\det(A)}
\]

\dfn{Sistema principale equivalente}{Sia \(AX = B\) un sistema compatibile, si dice sistema principale equivalente un sistema \(A'X = B'\) ottenuto eliminando \(m-p\) equazioni da \(AX=B\) tale che \(\rho (A'|B') = \rho (A') = p\).}
\thm{}{Un sistema \(AX = B\) compatibile ha le stesse soluzioni di un suo sistema principale equivalente.}

\paragraph{Osservazione:}\(\rho (A)=\rho (A|B)\) se il sistema lineare è omogeneo e quindi è sempre compatibile. In particolare \(X = 
\begin{pmatrix}
    0 \\
    \vdots \\
    0 \\
\end{pmatrix}
\) è sempre soluzione di \(AX = \ul{0} \).

\dfn{Autosoluzioni}{Le soluzioni di un sistema lineare omogeneo diverse dalla soluzione nulla si dicono  \textbf{autosoluzioni}.}
\newpage
\nt{Non è detto che un sistema lineare omogeneo ammetta autosoluzioni.}

\mprop{}{Un sistema lineare omogeneo \(AX = B = \ul{0} \) ammette autosoluzioni se, e solo se, \(\rho (A) < n\) (con \(n\) numero di incognite).}

\cor{}{Un sistema lineare omogeneo \(AX = B = \ul{0} \) con \(A \in M_n(K)\) ammette autosoluzioni se, e soltanto se,  \(\det (A) = 0\).}

\thm{}{Sia \(AX = \ul{0} \) un sistema lineare omogeneo con \(A \in K^{m,n}\) e sia \(S\) l'insieme delle sue soluzioni, allora \(S\) è un sottospazio di \(K^{n}\) di dimensione \(n-\rho (A)\).}

\paragraph{Osservazioni:} 
\begin{enumerate}
    \item \(\ul{0} \in S\) 
    \item se \(n-\rho (A) > 0\) abbiamo autosoluzioni 
    \item Se \(B \neq \ul{0} \) l'insieme delle soluzioni di \(AX = B\) non è un sottospazio di \(K^{n}\) perché \(A\ul{0} = \ul{0} \neq B \implies \{\ul{0} \} \notin S\).
\end{enumerate}

\mprop{}{Sia \(AX = B\) un sistema lineare in \(m\) equazioni ed \(n\) incognite, detto \(S\) l'insieme delle soluzioni abbiamo che \[
S =
\begin{cases}
    \ \{x_0+z : \ x_0 \in S, \ z \in S\}\text{ se }AX = B\text{ è compatibile } \\
    \ \emptyset \text{ se } AX = B \text{ non è compatibile} \\
\end{cases}
\] }

\dfn{Sistema lineare omogeneo associato}{Dato \(AX = B\) sistema lineare in \(m\) equazioni ed \(n\) incognite diciamo che \(AX = \ul{0} \) è il \textbf{sistema lineare omogeneo associato} a \(AX = B\).}

\mprop{}{Le soluzioni di un sistema lineare compatibile \(AX=B\) sono tutte e sole del tipo \(\overline{X}= X_0 + Z\), ove \(X_0\) è una soluzione particolare di \(AX = B\) e \(Z\) è la soluzione di \(AX = \ul{0} \), sistema omogeneo associato ad \(AX = B\).}

\pf{Dimostrazione}{Sia \(\overline{X}\) soluzione di \(AX=B\), poniamo \(Z = \overline{X}-X_0 \iff \overline{X}=X_0+Z\) \[
AZ = A(\overline{X}-X_0) = A\overline{X}- AX_0 = B- B = \ul{0} 
\] Quindi \(Z\) è soluzione del sistema lineare omogeneo associato ad \(A\). Di conseguenza \(\overline{X}= X_0 + Z\) }

Dato \(AX=B\) sistema lineare in \(m\) equazioni ed \(n\) incognite compatibile, le sue soluzioni sono tante quante quelle del sistema lineare omogeneo associato che costituiscono uno spazio vettoriale di dimensione \(n- \rho (A)\). Se il campo è infinito, posto \(\rho (A) = p\), si dice che le soluzioni sono \(\infty^{n-p}\) (cioè che l'insieme delle soluzioni dipende da \(n-\rho(A)\) parametri).

\newpage
\thm{}{Sia \(AX = \ul{0} \) un sistema lineare omogeneo in \(n\) incognite e sia \(\rho(A) = n-1\). Se si indica con \(A'X = \ul{0} \) un sistema principale equivalente ad \(AX = \ul{0} \) e si indicano con \(\Gamma_1, \Gamma_2, \ldots , \Gamma_n\) i determinanti dei minori di ordine \(n-1\), ottenuti eliminando in \(A'\) successivamente la prima, la seconda, \ldots , la n-esima colonna, allora le soluzioni del sistema sono, al variare di \(\lambda \in K\), \[
S = (\lambda \Gamma _1, -\lambda \Gamma _2, \ldots , (-1)^{n-1} \lambda \Gamma _n)
\] }

\section{Cambiamenti di base}
in uno spazio vettoriale \(V_n(K)\), di dimensione \(n\), siano \(B = (e_1,e_2, \ldots ,e_n)\) e \(B' = (e'_1,e'_2, \ldots ,e'_n)\) due basi assegnate. Ogni vettore della base \(B'\) si può esprimere come combinazione lineare dei vettori della base \(B\), cioè \[
\begin{cases}
    \ e_1' = a_{11}e_1+ a_{12}e_2+\ldots + a_{1n}e_n \\
    \ e_2' = a_{21}e_1+ a_{22}e_2+\ldots +a_{2n}e_n \\
    \ \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots  \\
    \ e_n'= a_{n 1}e_1+ a_{n 2}e_2 + \ldots + a_{nn}e_n \\
\end{cases}
\]con le seguenti posizioni \[
A = 
\begin{pmatrix}
    a_{11} & a_{12} & \ldots  & a_{1n} \\
    a_{21} & a_{22} & \ldots  & a_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{n1} & a_{n 2} & \ldots  & a_{nn} \\
\end{pmatrix}, \ E= 
\begin{pmatrix}
    e_1 \\
    e_2 \\
    \vdots \\
    e_n \\
\end{pmatrix} \text{ ed } E'=
\begin{pmatrix}
    e_1' \\
    e_2' \\
    \vdots \\
    e_n' \\
\end{pmatrix}
\] 
il sistema si può scrivere in forma compatta \[
E' = AE
\] 
\dfn{Matrice del cambiamento di base}{La matrice A si dice \textbf{matrice del cambiamento di base} da \(B\) a \(B'\).}

\mprop{}{La matrice \(A\) del cambiamento di base da \(B\) a \(B'\) è invertibile e \(A^{-1}=A'\).}
\pf{Dimostrazione}{\[
    E = A'E' = A'(AE) = (A'A)E \implies A'A=I_n \] \[
E' = AE = A(A'E') = (AA')E' \implies AA'=I_n
\] }

Stabiliamo il legame tra le componenti di uno stesso vettore \(v\), rispetto a due basi diverse \(B\) e \(B'\). Poniamo \[
X = \begin{pmatrix} x_1\\ \vdots\\ x_n \end{pmatrix} \text{ e } X'=\begin{pmatrix} x'_1\\ \vdots\\ x'_n \end{pmatrix}
\] Possiamo scrivere il generico vettore \(v \in V_n(K)\) \[
v = x_1e_1+x_2e_2 + \ldots + x_n e_n = (x_1,x_2, \ldots , x_n)E= {^{t}X}E \] \[
v = x_1'e_1'+x_2'e_2' + \ldots + x_n' e_n' = (x_1',x_2', \ldots , x_n')E= {^{t}X'}E'
\] \[
v = {^{t}X}E = {^{t}X'}E
\] Sostituendo si ha \({^{t}X}E = {^tX'}AE\), ove \(A\) è la matrice del cambiamento di base da \(B\) a \(B'\), quindi, dato che le componenti dei vettori sono univocamente determinate \[
X = {^tA}X'
\] \[
X' = {^tA^{-1}}X
\] Possiamo dire quindi che le componenti di uno stesso vettore rispetto a due basi \(B\) e \(B'\) sono legate dalla matrice del cambiamento di base da \(B\) a \(B'\).

\chapter{Autovalori, autovettori e diagonalizzabilità}
\section{Ricerca di autovalori, polinomio caratteristico}
\dfn{Polinomio ed equazione caratteristica}{Se \(A\) è una matrice quadrata di ordine \(n\), si dice \textbf{polinomio caratteristico} di \(A\), e si indica \(p_A(\lambda )\), il determinante della matrice \(A-\lambda I_n\), cioè \[
p_A(\lambda ) = |A-\lambda I_n| 
\] L'equazione \(p_A(\lambda) = |A -\lambda I_n| \) è detta \textbf{equazione caratteristica} di \(A\).}

\dfn{Autovalori}{Le radici del polinomio caratteristico si chiamano \textbf{autovalori} di \(A\).}
\dfn{Autospazio}{Lo spazio delle soluzioni del sistema \((A-\overline{\lambda }I_n)X=0\), dove \(\overline{\lambda }\) è un autovalore, si chiama \textbf{autospazio} associato a \(\overline{\lambda }\) e si indica con \(V_{\overline{\lambda }}\).}
\dfn{Autovettori}{I vettori non nulli dell'autospazio \(V_{\overline{\lambda }}\) si chiamano  \textbf{autovettori} relativi a \(\overline{\lambda }\).}
\paragraph{Osservazione:} Si potrebbe dimostrare che se il polinomio caratteristico di \(A \in M_n(K)\) ha grado \(n\) allora gli autovalori di \(A\) sono al massimo \(n\).
\dfn{Matrici simili}{Due matrici \(A,B \in M_n(K)\) si dicono \textbf{simili} se esiste \(P \in M_n(K)\) con \(|P| \neq 0\) tale che  \[
B = P^{-1}AP \quad PB = AP
\] }

\mprop{}{Due matrici simili \(A,B\) hanno lo stesso determinante e lo stesso polinomio caratteristico (e di conseguenza gli stessi autovalori).}
\pf{Dimostrazione}{Per ipotesi le due matrici \(A,B\) sono simili quindi:\[
\exists P \in M_n(K), \ |P| \neq 0 : \ B = P^{-1}AP
\] \[
|B| = |P^{-1}AP| = |P^{-1}| |A| |P| = \frac{1}{|P| }|A| |P| =|A| \implies |B| = |A| 
\] \[
p_B(\lambda ) = |B - \lambda I_n| = |P^{-1}AP - \lambda P^{-1}I_n P| = |P^{-1}(A - \lambda I_n)P| = \frac{1}{|P| }|A-\lambda I_n| |P| = |A - \lambda I_n| = p_A(\lambda )
\] e attraverso questa serie di passaggi abbiamo potuto dimostrare che se due matrici sono simili allora avranno sia lo stesso determinante che lo stesso polinomio caratteristico. }

\section{Matrici diagonalizzabili}
\dfn{Matrice diagonalizzabile}{Una matrice \(A \in M_n(K)\) si dice \textbf{diagonalizzabile} se è simile ad una matrice diagonale, ovvero esistono \(D, P \in M_n(K)\) con \(D\) matrice diagonale, \(|P| \neq 0\) e \(D = P^{-1}AP\).}

\thm{Primo criterio di diagonalizzabilità}{Una matrice \(A \in M_n(K)\) è diagonalizzabile se, e soltanto se, \(K^{n}\) ammette una base costituita da autovettori di \(A\).}
\pf{Dimostrazione}{\("\implies "\) Per ipotesi \(A\) è diagonalizzabile quindi \(\exists \ D,P \in M_n(K): D\) è diagonale \(|P| \neq 0\) e \(PD = AP\). Per semplicità denotiamo le colonne di \(P= 
\begin{pmatrix}
    P_1 & P_2 & \ldots  & P_n \\
\end{pmatrix}
\). \[
AP = A 
\begin{pmatrix}
    P_1 & P_2 & \ldots  & P_n \\
\end{pmatrix} =
\begin{pmatrix}
    AP_1 & AP_2 & \ldots  & AP_n \\
\end{pmatrix}
\] \[
PD = 
\begin{pmatrix}
    P_1 & P_2 & \ldots  & P_n \\
\end{pmatrix} 
\begin{pmatrix}
    d_1 & 0 & \ldots  & 0 \\
    0 & d_2 & \ldots  & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \ldots  & d_n \\
\end{pmatrix} = 
\begin{pmatrix}
    d_1P_1 & d_2P_2 & \ldots  & d_nP_n \\
\end{pmatrix}
\] Quindi \[
\begin{pmatrix}
    AP_1 & AP_2 & \ldots  & AP_n \\
\end{pmatrix} = 
\begin{pmatrix}
    d_1P_1 & d_2P_2 & \ldots  & d_nP_n \\
\end{pmatrix}
\iff 
AP_1=d_1P_1, \ AP_2 = d_2P_2, \ \ldots, \ AP_n = d_n P_n
\] \[
\implies AX = \lambda X \quad \lambda =d_i \quad X = P_i
\] dove \(d_i\) è un autovalore, \(P_i\) è un autovettore di \(A\) e \(
\begin{pmatrix}
    P_1 & P_2 & \ldots  & P_n \\
\end{pmatrix} 
\) è una sequenza di \(n\) autovettori. Poiché \(\dim K^{n}=n\) e la sequenza è composta da \(n\) vettori, è sufficiente controllare la lineare indipendenza di \(P\). Ma siccome avevamo supposto per ipotesi che \(|P| \neq 0\) le sue \(n\) colonne sono linearmente indipendenti. Quindi \(B = (P_1, P_2, \ldots , P_n)\) è una base di \(K^{n}\) costituita da autovettori di \(A\).

"\(\impliedby \)" è analogo, basta ripercorrere il ragionamento a ritroso.}

\paragraph{Osservazione:} Se \(A \in M_n(K)\) è diagonalizzabile allora:
\begin{itemize}
    \item \(D\) ha sulla diagonale principale gli autovalori di \(A\);
    \item \(P\), cioè la matrice diagonalizzante, ha nelle colonne gli autovettori della base di \(K^{n}\).
\end{itemize}

\dfn{Molteplicità algebrica e geometrica}{Sia \(\overline{\lambda }\) un autovalore di \(A \in M_n(K)\); si chiama:
\begin{itemize}
    \item \textbf{molteplicità algebrica} di \(\overline{\lambda }\) il numero di volte che \(\overline{\lambda }\) è radice del polinomio caratteristico, e si indica con \(a_{\overline{\lambda }}\) 
    \item \textbf{molteplicità geometrica} di \(\overline{\lambda }\) la dimensione dell'autospazio \(V_{\overline{\lambda }}\) associato a \(\overline{\lambda }\), e si indica con \(g_{\overline{\lambda }}\).
\end{itemize}}

\mprop{}{Sia \(\overline{\lambda }\) un autovalore di \(A \in M_n(K)\). Allora \[
1 \le g_{\overline{\lambda }} \le a_{\overline{\lambda }}
\] }

\mprop{}{Sia \(A \in M_n(K)\) e siano \(\lambda _1, \lambda _2, \ldots ,\lambda_n\) \(t\) autovalori di \(A\) distinti tra loro, allora la somma dei relativi autospazi è diretta. \[
V_{\lambda _1} \oplus V_{\lambda _2} \oplus \ldots \oplus V_{\lambda _t}
\] }
\paragraph{Osservazioni:} 
\begin{enumerate}
    \item \(A \in M_n(K) \implies \deg(p_A(\lambda )) = n\), quindi ho al massimo \(n\) autovalori;
    \item \(\sum a_{\lambda _i}\le n\);
    \item \(\sum a_{\lambda _i}= n \iff\) tutti gli autovalori di \(A\) sono in \(K\);
    \item \(S =V_{\lambda _1} \oplus V_{\lambda _2} \oplus \ldots \oplus V_{\lambda _t} \implies \dim S = \sum \dim V_{\lambda _i} = \sum g_{\lambda _i}\)
    \item Autovettori provenienti da autospazi diversi sono tra loro linearmente indipendenti (perché la somma è diretta).
\end{enumerate}

\thm{Secondo criterio di diagonalizzabilità}{Sia \(A \in M_n(K)\) e siano \(\lambda _1, \lambda _2, \ldots , \lambda _n\) gli autovalori distinti di \(A\). Allora \(A\) è diagonalizzabile se, e soltanto se:
\begin{enumerate}
    \item tutti gli autovalori di \(A\) sono in \(K\);
    \item Per ogni autovalore vale \(a_{\lambda _i} = g_{\lambda _i}\)(e allora si dice che l'autovalore è regolare).
\end{enumerate}}

\pf{Dimostrazione}{"\(\implies\)" Per ipotesi \(A\) è diagonalizzabile. Per il primo criterio di diagonalizzabilità \(K^{n}\) ammette una base \(B\) formata da autovettori, cioè tale che  \(\mcL(B) = K^{n}\) e \(B \subseteq V_{\lambda _1} \oplus V_{\lambda _2} \oplus \ldots \oplus V_{\lambda _t} \le K^{n}\). Quindi \[
K^{n} = \mcL(B) \le \mcL(V_{\lambda _1} \oplus V_{\lambda _2} \oplus \ldots \oplus V_{\lambda _t}) = V_{\lambda _1} \oplus V_{\lambda _2} \oplus \ldots \oplus V_{\lambda _t} \le K^{n} 
\] \[
\implies V_{\lambda _1} \oplus V_{\lambda _2} \oplus \ldots \oplus V_{\lambda _t} = K^{n} \]
\[
\implies n = \dim K^{n} = \dim (V_{\lambda _1} \oplus V_{\lambda _2} \oplus \ldots \oplus V_{\lambda _t}) = \sum g_{\lambda _i}\le \sum a_{\lambda _i} \le n
\]
Siccome  \(\sum a_{\lambda _i}=n\) tutti gli autovalori di \(A\) sono in \(K\). Inoltre \(\sum g_{\lambda _i} = \sum a_{\lambda _i}\) e \(g_{\lambda _i}\le a_{\lambda _i}\implies a_{\lambda _i}=g_{\lambda _i}\).

"\(\impliedby \)" Per ipotesi abbiamo che tutti gli autovalori di \(A\) soni in \(K\) e per ogni autovalore vale \(a_{\lambda _i}= g_{\lambda _i}\). Per ogni autovalore \(\overline{\lambda }\) avremo un relativo autospazio a cui corrisponde una relativa base di autovettori  \(B_1, B_2, \ldots , B_t\). Chiamiamo \(B = \bigcup_{i = 1}^t B_i \), cioè l'unione di tutte le basi. Certamente \(B\) è libera perché la somma di sottospazi distinti è diretta. \[
|B| = |\bigcup B_i | = \sum |B_i| = \sum \dim V_{\lambda _i}= \sum g_{\lambda _i}= \sum a_{\lambda _i}=n
\] Quindi \(B\) è una base di \(K^{n}\) costituita da autovettori e per il primo criterio di diagonalizzabilità \(A\) è diagonalizable.}

\chapter{Forme bilineari e prodotti scalari}
\section{Forme bilineari}
\dfn{Forma bilineare e prodotto scalare}{Sia \(V_n(K)\) uno spazio vettoriale. Una \textbf{forma bilineare} in \(V\) è una funzione \(*: \ V \times V \to K: \)
\begin{itemize}
    \item \((u+v) * w = u * w + v * w \qquad \forall u, v, w \in V \ \forall k \in K\)
    \item \(u * (v + w) = u * v + u * w \qquad \forall u, v, w \in V \ \forall k \in K\)
    \item \((ku) * v = u * (kv) = k (u * v) \qquad \forall u, v, w \in V \ \forall k \in K\) 
\end{itemize} 
Se poi \(*\) verifica anche l'ulteriore proprietà
\begin{itemize}
    \item \(v * w = w * v \qquad \forall u, v, w \in V \ \forall k \in K\)
\end{itemize}
Allora si chiama \textbf{prodotto scalare} (o forma bilineare simmetrica).}
\paragraph{Osservazione:} Si deduce chiaramente che \(\forall v \in V \quad \ul{0} * v = 0 = v * \ul{0} \).

\ex{Prodotto scalare euclideo e standard}{
\begin{enumerate}
    \item Definiamo il \textbf{prodotto scalare euclideo} come una funzione \(*:\RR^{n} \times \RR^{n} \to \RR: \) \[
     (x_1, x_2, \ldots , x_n)*(x_1', x_2', \ldots , x_n') = x_1x_1'+x_2x_2'+\ldots +x_nx_n'
    \] 
    \item Definiamo il \textbf{prodotto scalare standard} come la funzione \(*: M_n(\RR) \times M_n(\RR) \to \RR:\) \[
\begin{pmatrix}
    x_{11} & x_{12} & \ldots  & x_{1n} \\
    x_{21} & x_{22} & \ldots  & x_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    x_{n 1} & x_{n 2} & \ldots  & x_{n n} \\
\end{pmatrix} \ * \
\begin{pmatrix}
    x_1' & x'_{12} & \ldots  & x'_{1n} \\
    x'_{21} & x'_{22} & \ldots  & x'_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    x'_{n 1} & x'_{n 2} & \ldots  & x'_{n n} \\
\end{pmatrix} = x_{11}x_{11}'+x_{12}x_{12}'+ \ldots + x_{n n}x_{n n}'
    \] 
\end{enumerate}
}

\section{Prodotti scalari e ortogonalità}
\dfn{Ortogonalità}{In uno spazio vettoriale \(V(K)\), con prodotto scalare "\(\cdot \)", due vettori \(v\) e \(w\) di \(V\) si dicono \textbf{ortogonali}, e si scrive \(v \perp w\), se \(v \cdot w = 0\).}
\dfn{Complemento ortogonale}{Sia \(V(K)\) uno spazio vettoriale e "\(\cdot \)" un prodotto scalare. Sia \(\emptyset \neq A \subseteq V\); si chiama  \textbf{complemento ortogonale} (o più semplicemente ortogonale) di \(A\) l'insieme \[
A^{\perp} = \{v \in V : \ v \perp w, \ \forall w \in A\} \qquad \ul{0} \in A^{\perp} \neq \emptyset
\] }

\mprop{}{Sia \(V(K)\) uno spazio vettoriale con prodotto scalare "\(\cdot \)". Sia \(\emptyset \neq A \subseteq V\). Allora \(A^{\perp}\) è un sottospazio vettoriale.}
\pf{Dimostrazione}{Sappiamo che \(\ul{0} \in A^{\perp} \neq \emptyset\) \\ 
Dobbiamo dimostrare che \[
\forall u_1, u_2 \in A^{\perp}, \ \forall k_1,k_2 \in K \qquad k_1u_1+k_2u_2 \in A^{\perp}
\] Possiamo scrivere per la proprietà di ortogonalità che\[
\forall w \in A \quad u_1\cdot w = 0 \quad u_2 \cdot w = 0
\] Quindi \[
(k_1u_1+k_2u_2) \cdot w = (k_1u_1) \cdot w + (k_2u_2) \cdot w = k_1 (\underbrace{u_1 \cdot w}_{=0}) + k_2 (\underbrace{u_2 \cdot w}_{=0} )
\] \[
\implies k_1u_1+k_2u_2 \in A^{\perp} \implies A^{\perp} \ \text{è un sottospazio.}
\] 
}

\paragraph{Osservazioni:} 
\begin{enumerate}
    \item \(A \subseteq B \implies A^{\perp} \supseteq B^{\perp}\)
    \item \(A^{\perp} = [\mcL(A) ]^{\perp}\)
    \item Generalmente se \(A \le V(K) \implies A \neq (A^{\perp})^{\perp}\), ma \(A \subseteq (A^{\perp})^{\perp}\)
\end{enumerate}

\mprop{}{Sia \(V_n(K)\) uno spazio vettoriale con prodotto scalare "\(\cdot \)" e siano \(v, w \in V(K)\) con \(w \cdot w \neq \ul{0} \). Allora \[
\exists \ v_1, v_2 \in V: \ v = v_1+v_2, \ v_1 = kw, \ v_2 \perp w
\] }

\pf{Dimostrazione}{ \[
k = \frac{v \cdot w}{w \cdot w} \qquad v_1= kw = \left( \frac{v \cdot w}{w \cdot w} \right) \cdot w
\] \[
v_2=v-v_1 \iff v_1+v_2=v
\] Ora verifichiamo che \(v_2 \perp w\) \[
v_2 \perp w \iff  (v-v_1) \cdot w = \left( v - \frac{v \cdot  w}{w \cdot w} \right) \cdot  w = v - w - \frac{v \cdot w}{w \cdot w}\cdot w \cdot w = v \cdot w - v \cdot w = 0
\] }
\dfn{Coefficiente di Fourier e proiezione}{Sia \(V_n(K)\) uno spazio vettoriale con prodotto scalare "\(\cdot \)" e siano \(v, w \in V(K)\) con \(w \cdot w \neq \ul{0} \). Allora \[
    k = \frac{v \cdot w}{w \cdot w}
\] si chiama \textbf{coefficiente di Fourier} di \(v\) lungo \(w\) e \[
v_1= \frac{v \cdot w}{w \cdot w}\cdot w
\] si chiama \textbf{proiezione} di \(v\) lungo \(w\).}

\dfn{Forma quadratica}{Sia \(V_n(K)\) uno spazio vettoriale con prodotto scalare "\(\cdot \)" e sia \(v \in V(K)\). Si chiama \textbf{forma quadratica} associata a "\(\cdot \)" la funzione \[
q: \ 
\begin{cases}
    \ V \to K  \\
    \ v \mapsto q(v) = v \cdot v \\
\end{cases}
\] }

\section{Spazi con prodotto scalare definito positivo}
\dfn{Prodotto scalare definito positivo}{Sia \(V(K)\) uno spazio vettoriale su campo \(K\) \ul{ordinato}. Un prodotto scalare "\(\cdot \)" in \(V\) si dice \textbf{definito positivo} se \(\) \[
\forall v \in V \quad v \cdot v \ge 0 \quad e \quad v \cdot v = 0 \iff v = \ul{0} 
\] Per chiarezza da qui in avanti quando si parla di prodotti scalari definiti positivi \(K = \RR \) in modo tale che esso sia ordinato. Di conseguenza denotiamo con \textbf{spazio vettoriale metrico reale} \(V_n^{\circ}(\RR )\), cioè uno spazio vettoriale dotato di un prodotto scalare definito positivo.}

\dfn{}{Dato \(V^{\circ}_n(\RR)\) si chiama  \textbf{norma} la funzione \[
\|\cdot \|: \ 
\begin{cases}
    \ V \to \RR \\
    \ v \mapsto \|v\|=\sqrt{v \cdot v} = \sqrt{q(v)} \\
\end{cases}
\] }
\ex{Vettori geometrici}{\[
\vec{v} \cdot \vec{w} = |\vec{v} | |\vec{w}| \cos \alpha 
\] \[
\|\vec{v}\|=\sqrt{\vec{v} \cdot \vec{v}} = \sqrt{|\vec{v}| |\vec{v}| \cos 0} = \sqrt{|\vec{v}| ^2} = |\vec{v}| 
\] }

\paragraph{Osservazioni:}

\begin{enumerate}
    \item La norma generalizza la nozione di "lunghezza" di un vettore.
    \item \(\|v\|= \ul{0} \iff v \cdot v = 0 \iff v = \ul{0} \) 
\end{enumerate}
\mprop{}{In \(V^{\circ}_n(\RR )\) valgono i seguenti fatti
\begin{enumerate}
    \item \(\|v\|\ge 0 \quad e\quad \|v\|=0 \iff v = \ul{0} \) 
    \item \(\|kv\|=|k|\|v\| \) 
    \item \(|v \cdot w| \le \|v\|\cdot \|w\|\) (disuguaglianza di Schwarz)
    \item \(\|v+w\|\le \|v\|+\|w\|\) (disuguaglianza triangolare)
\end{enumerate}}

\paragraph{Osservazioni:} Sia "\(\cdot \)" un prodotto scalare euclideo definito su \(\RR ^{n}(\RR )\). La sua base canonica è \[B_c = ((1,0,\ldots , 0), (0,1,0, \ldots , 0), \ldots , (0,0, \ldots , 0, 1)) = (e_1, e_2, \ldots , e_n)\] 
\begin{enumerate}
    \item \(\|e_i\|=\sqrt{e_i \cdot e_i} = 1\) 
    \item \(e_i \cdot e_j = 0 \quad \forall i \neq j \implies e_i \perp e_j\) 
    \item \(\forall \underbrace{(x_1, x_2, \ldots , x_n)}_{=v}  = x_1(1, 0, \ldots , 0) + x_2 (0,1, 0 , \ldots , 0) + \ldots + x_n(0,0, \ldots , 0, 1)\) \\ \(\implies v \cdot e_i = x_i =\) i-esima componente di \(v\) rispetto a \(B_c\)
\end{enumerate}

\dfn{Base ortogonale e ortonormale}{I vettori \(v_1, v_2, \ldots , v_n\) di uno spazio vettoriale \(V^{\circ}_n(\RR )\) formano un insieme \textbf{ortogonale} se \(v_i \cdot v_j = 0, \ i \neq j\). Se inoltre ciascuno dei \(v_i\) ha norma unitaria, allora parleremo di insieme  \textbf{ortonormale}. Se poi tali vettori costituiscono una base di \(V^{\circ}_n(\RR )\) parleremo di base ortogonale o ortonormale.}

\mprop{}{Se \(\emptyset\neq A \subseteq V^{\circ}_n(\RR )\) e costituito da vettori tutti non nulli. Allora \(A\) è libero.}
\pf{Dimostrazione}{\[A = \{v_1,v_2, \ldots , v_n\} \quad v_i \cdot v_j = 0 \quad \forall i \neq j. \quad  \text{Siano} \ \alpha _1, \alpha _2, \ldots , \alpha _n \in \RR : \ \alpha _1 v_1 + \alpha _2v_2 + \ldots + \alpha _n v_n = \ul{0} \]
\[
0 = 0 \cdot v_1=(\alpha _1, \alpha _2, \ldots , \alpha _n) \cdot v_1 = \alpha _1 \underbrace{v_1 \cdot v_1}_{\neq 0 \implies v_1 \neq \ul{0} \implies \|v_1\|^2 \neq 0}  + \alpha _2 \underbrace{v_2 \cdot v_2}_{=0}  + \ldots + \alpha _n \underbrace{v_n \cdot v_n}_{= 0}  = \underbrace{\|v_1\|^2}_{\neq 0} \underbrace{\alpha _1}_{\implies \alpha _1=0} 
\] Ripeto il ragionamento per ciascuno dei \(v_i\) e ottengo che gli unici \(\alpha \) che mi danno il vettore nullo sono quelli tutti nulli. Quindi se \(\alpha _1 = \alpha _2 = \ldots = \alpha _n = 0 \implies A\) è libero.
}

\paragraph{Osservazione:} In \(V^{\circ}_n(\RR )\) se \(A\) è un insieme ortogonale di \(n\) vettori tutti diversi dal vettore nullo allora \(A\) è libero. Dunque fissato un ordine abbiamo una base ortogonale.

\thm{Processo di ortogonalizzazione di Gram-Schmidt}{Siano \(V^{\circ}_n(\RR )\) e \(B = (e_1, e_2, \ldots , e_n)\) una base. La sequenza \(B' = (e_1', e_2', \ldots , e_n')\) così definita \[
e_1'=e_1
\] \[
e_2'=e_2-\frac{e_2\cdot e_1'}{e_1'\cdot e_1'}\cdot e_1'
\] \[
e_3'=e_3-\frac{e_3\cdot e_1'}{e_1'\cdot e_1'}\cdot e_1'-\frac{e_3\cdot e_2'}{e_2'\cdot e_2'}\cdot e_2'
\] \[
\vdots
\] \[
e_n' = e_n - \frac{e_n \cdot e_1'}{e_1'\cdot e_1'}\cdot e_1'- \ldots - \frac{e_n \cdot e_{n-1}'}{e_{n-1}'\cdot e_{n-1}'}\cdot e'_{n-1}
\] è una base ortogonale di  \(V^{\circ}_n(\RR )\).}
\paragraph{Osservazione:} Se i primi \(p\) vettori di \(B\) sono già ortogonali tra loro il metodo di Gram-Schmidt non li cambia.

\thm{}{Se \(A\) è un sottoinsieme non vuoto di \(V^{\circ}_n(\RR )\), la cui copertura non coincide con \(V^{\circ}_n(\RR )\), allora \[
V^{\circ}_n(\RR ) = \mcL(A) \oplus A^{\perp}
\] }
\pf{Dimostrazione}{Prima di tutto dimostriamo che \(\mcL(A) \cap A^{\perp} = \{\ul{0} \} \) infatti: \(v \in \mcL(A) \cap A^{\perp}\) e se \(v \in A^{\perp}=[\mcL(A) ]^{\perp} \quad v \cdot v = 0 \implies v = \ul{0} \) poiché ci troviamo in un prodotto scalare definito positivo. Quindi la somma è diretta. Ora si può dimostrare che \(\mcL(A) \oplus A^{\perp}=V^{\circ}_n(\RR )\). Sia \(\dim \mcL(A) = p\) e sia \(B = (v_1, v_2, \ldots , v_p)\) una base ortogonale di \(\mcL(A) \); per il teorema di completamento ad una base possiamo completare \(B\) ad una base di \(V^{\circ}_n(\RR )\). Aggiungiamo a \(B\) \(n-p\) vettori. Ora applichiamo a tale base il processo di ortogonalizzazione di Gram-Schmidt.  \(B' = (v_1 , \ldots , v_p , v_{p+1}', \ldots , v_n')\) è una base ortogonale di \(V_n^{\circ}(\RR )\). Quindi \(\mcL(B') = V_n^{\circ}(\RR )\). Ora tutti i vettori aggiunti sono ortogonali ai vettori originali, cioè \(v_{p+1}', \ldots , v_n' \in \mcL(A)^{\perp} = A^{\perp} \implies \mcL(A) \oplus A^{\perp} = V^{\circ}_n(\RR ) \).}

\paragraph{Osservazioni:} 
\begin{enumerate}
    \item \(A^{\perp}\) è un complemento diretto di \(\mcL(A) \) 
    \item Per la formula di Grassmann abbiamo che \[
    n = \dim (\mcL(A) \oplus A^{\perp}) = \dim \mcL(A) + \dim A^{\perp} \implies \dim A^{\perp} = n-\dim \mcL(A) 
    \] 
    \item Per il punto precedente possiamo affermare che se il prodotto scalare è definito positivo allora \(U \le V_n^{\circ}(\RR ) \implies U = (U^{\perp})^{\perp}\)
\end{enumerate}

\thm{}{L'insieme delle soluzioni di un sistema lineare omogeneo è un sottospazio vettoriale di \(\dim : \ n- \rho(A) \)}
\pf{Dimostrazione}{ In \(\RR ^{n}\) con prodotto scalare euclideo \[
(x_1,x_2, \ldots , x_n) \cdot (x_1', x_2', \ldots , x_n') = x_1x_1' + x_2x_2' + \ldots + x_n x_n'
\] Quindi possiamo riscrivere il sistema come
\[
\begin{cases}
    \ a_{11}x_1+\ldots +a_{1n}x_n = 0 \\
    \ \ldots \ldots \ldots \ldots \ldots  \ldots \ldots \ldots  \\
    \ a_{m 1}x_1+\ldots +a_{mn}x_n = 0 \\
\end{cases} \iff
\begin{cases}
    \ (a_{11}, \ldots , a_{1n}) \cdot (x_1, \ldots , x_n) = 0 \\
    \ \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots \ldots  \\
    \ (a_{m1}, \ldots , a_{mn}) \cdot (x_1, \ldots , x_n) = 0 \\
\end{cases}
\] Pensando alle righe di \(A\) come vettori di \(\RR ^{n}\) le equazioni del sistema esprimono il fatto che il prodotto scalare di tali righe per il generico vettore \((x_1, x_2, \ldots , x_n)\) è uguale a zero. Quindi il generico vettore è ortogonale a tutte le righe di \(A\). Chiamando \(\mcL(R) \) lo spazio generato dalle righe di \(A\). L'insieme \(S\) delle soluzioni di \(AX = \ul{0} \) coincide con \(\mcL(R) ^{\perp}\). E quindi per il teorema di Kronecker \(\dim S = n - \dim \mcL(R) = n - \rho(A) \).}

\section{Matrici di forme bilineari}

\dfn{Matrice di forma bilineare}{Sia \(V_n(K)\) uno spazio vettoriale, "\(*\)" una forma bilineare e \(B=(e_1,e_2, \ldots , e_n)\) base di \(V_n(K)\). Si chiama \textbf{matrice della forma bilineare} "\(*\)" rispetto a \(B\) \[
A^{*}_B =
\begin{pmatrix}
    e_1*e_1 & e_1*e_2 & \ldots  & e_1*e_n \\
    e_2*e_1 & e_2*e_2 & \ldots  & e_2*e_n \\
    \vdots & \vdots & \ddots & \vdots \\
    e_n*e_1 & e_n*e_2 & \ldots  & e_n*e_n \\
\end{pmatrix} \in M_n(K)
\] Si può indicare in modo più compatto con \[
A^{*}_B = (e_i * e_j)
\] }
\nt{La matrice di una forma bilineare dipende dalla base fissata.}

\mprop{}{La matrice che rappresenta un prodotto scalare rispetto a una base qualsiasi è simmetrica.}

\pf{Dimostrazione}{\(B = (e_1, e_2, \ldots , e_n)\) e "\(\cdot \)" è il prodotto scalare. Allora \(A^{\cdot }_B = (e_i \cdot e_j) = (e_j \cdot e_i) = {^tA^{\cdot }_B}\).}

\mprop{}{Sia "\(\cdot \)" un prodotto scalare su \(V_n(K)\) e sia \(B\) una sua base. Sia \(A^{\cdot }_B\) una matrice associata a "\(\cdot \)" rispetto alla base \(B\). Allora 
\begin{itemize}
    \item \(B\) è ortogonale \(\iff A^{\cdot }_B\) è diagonale \[
    e_i \cdot e_j = 0 \quad \forall i \neq j \iff a_{ij} = 0 \quad \forall i \neq j
    \] 
    \item \(B\) è ortonormale \(\iff A^{\cdot }_B = I_n \in M_n(K)\) \[
    e_i \cdot e_j = 0 \quad \forall i \neq j \quad e \quad e_i \cdot  e_i = 1 \quad \forall 1 \le i \le n \iff a_{ij} = 0 \quad \forall i \neq j \quad e\quad a_{ii} = 1 \quad \forall 1 \le i \le n
    \] 
\end{itemize}}

\paragraph{Osservazione:} Utilizzando la matrice associata ad una forma bilineare "\(*\)" è possibile calcolare \[v * w \quad \forall v, w \in V_n(K)\].

\mprop{}{Sia \(B\) una base di \(V_n(K)\) e sia "\(*\)" una forma bilineare su \(V\). Dette \[
X = \begin{pmatrix} x_1\\ \vdots\\ x_n \end{pmatrix} \quad e \quad  Y = \begin{pmatrix} y_1\\ \vdots\\ y_n \end{pmatrix}
\] le matrici colonne delle componenti rispettivamente di \(v\) e di \(w \in V\) risulta: \[
v * w = {^tX} A^{*}_B Y
\] }

\section{Matrici ortogonali e basi ortonormali}
\dfn{Matrice ortogonale}{Sia \(A \in M_n(K)\) diciamo che \(A\) è \textbf{ortogonale} se \({^tA}= A ^{-1}\). Quindi \[A {^tA}= {^tA}A = I_n\]}

\mprop{}{Sia \(A \in M_n(K)\) una matrice ortogonale. Allora \(|A|  \in \{-1, 1\} \)}

\pf{Dimostrazione}{\[
|I_n| = 1 = |A A ^{-1}| = |A {^tA}| = |A| |{^tA}| = |A| |A| = |A| ^2
\] \[
|A| ^2 = 1 \iff |A| = \pm 1
\] }

\mprop{}{Sia \(A \in M_n(K)\). \(A\) è ortogonale se, e soltanto se, le sue righe (o colonne) costituiscono una base ortonormale di \(\RR ^{n}(\RR )\) rispetto al prodotto scalare euclideo (dello spazio euclideo \(\RR ^{n}(\RR )\)).}

\pf{Dimostrazione}{"\(\implies \)" \[
\begin{pmatrix} R_1\\ \vdots\\ R_n \end{pmatrix} \iff  {^tA}= ({^tR_1}, \ldots , {^tR_n})
\] \[
A {^tA} = I_n = \begin{pmatrix} R_1\\ \vdots\\ R_n \end{pmatrix}({^tR_1}, \ldots , {^tR_n}) = 
\begin{pmatrix}
    R_1 \cdot R_1 & R_1 \cdot R_2 & \ldots  & R_1 \cdot R_n \\
    R_2 \cdot R_1 & R_2 \cdot R_2 & \ldots  & R_2 \cdot R_n \\
    \vdots & \vdots & \ddots & \vdots \\
    R_n \cdot R_1 & R_n \cdot R_2 & \ldots  & R_n \cdot R_n \\
\end{pmatrix}
\] \[
R_i \cdot R_j = 0 \quad \text{se} \quad i \neq j, \quad R_i \cdot R_i = 1 \quad \forall 1 \le i \le n
\] Quindi le righe di \(A\) sono una base ortonormale. Il ragionamento è completamente analogo per le colonne. \\ "\(\impliedby \)" Si può dimostrare ripercorrendo le implicazioni al contrario.} 

\section{Matrici reali simmetriche}
\thm{}{Sia \(A \in M_n(\RR )\) simmetrica allora 
\begin{enumerate}
    \item Gli autovalori di \(A\) sono tutti reali (teorema spettrale)
    \item Gli autovettori di \(A\) relativi ad autospazi distinti sono ortogonali tra loro
\end{enumerate}}

\pf{Dimostrazione del punto 2}{Siano \(x\) e \(y\) autovettori relativi ad autovalori \(\lambda \) e \(\mu\) distinti. Quindi \(AX = \lambda x\) e \(AX = \mu y\). Sia \(\lambda \neq 0\). Quindi \[
        ({^tx}{^ty}) \lambda = (\lambda {^tx})y = {^t(x\lambda )y = {^t(Ax)y = \underbrace{({^tx}{^tA})y = ({^tx}A)y}_{\text{per la simmetria di \(A\)}}} = {^tx}(Ay)}
\] \[
= {^tx}\mu y = \mu ({^tx}y) = \mu ({^tx}{^ty}) \implies ({^tx}{^ty})\lambda = ({^tx}{^ty}) \mu
\] \[
\lambda k = \mu k \iff (\lambda - \mu) k = 0 \iff \mu = \lambda \quad \text{oppure}\quad {^tx}{^ty}=0
\] ma \(\mu \neq \lambda \) perché \(x\) e \(y\) stanno in autospazi distinti \(\implies {^tx}{^ty}=0 \implies x\) e \(y\) sono ortogonali.}

\cor{}{Una matrice reale e simmetrica di ordine \(n\) ammette \(n\) autovalori contati con la loro molteplicità algebrica.}

\dfn{Matrice ortogonalmente diagonalizzabile}{Data \(A \in M_n(K)\) è detta \textbf{ortogonalmente diagonalizzabile} se esistono \(D\), matrice diagonale di ordine \(n\), e \(P\) matrice ortogonale di ordine \(n\) tali che \[
D = P^{-1}AP = {^tP}AP
\] }
\thm{}{I seguenti fatti sono equivalenti
\begin{enumerate}
    \item \(A\in M_n(\RR )\) è ortogonalmente diagonalizzabile;
    \item \(\RR ^{n}\) ammette una base ortonormale di autovettori di \(A\);
    \item \(A\) è una matrice reale e simmetrica.
\end{enumerate}}

\end{document}
